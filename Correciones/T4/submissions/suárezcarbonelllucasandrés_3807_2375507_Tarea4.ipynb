{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-BRLgz3fB_l",
        "outputId": "d4603c3f-0754-4b0c-fd80-322ed99f164e"
      },
      "source": [
        "!pip install pyopencl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyopencl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/12/7d4171ecfaf61bafdc4a628263d086b8e75ff89f4ada5458ff1fd16d953c/pyopencl-2020.3.1-cp36-cp36m-manylinux1_x86_64.whl (738kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 9.3MB/s \n",
            "\u001b[?25hCollecting pytools>=2017.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pyopencl) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyopencl) (1.18.5)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2017.6->pyopencl) (1.15.0)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2017.6->pyopencl) (0.8)\n",
            "Building wheels for collected packages: pytools\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=93cb2c3538a6580827cc5656e4bbf6940f3fd7bb76f117296f568ab5c577afd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pytools\n",
            "Installing collected packages: appdirs, pytools, pyopencl\n",
            "Successfully installed appdirs-1.4.4 pyopencl-2020.3.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6bhXWaVkJ0y"
      },
      "source": [
        "import time as tm\n",
        "import numpy as np\n",
        "import pyopencl as cl\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uCqo1yUfK_x",
        "outputId": "d03af748-dd76-4b74-fc11-4c0872bae832"
      },
      "source": [
        "K = 10000\n",
        "localSize = 80\n",
        "globalSize = math.ceil(K/localSize) * localSize\n",
        "groups = math.ceil(K/localSize)\n",
        "\n",
        "\n",
        "L = 2\n",
        "np.random.seed(12)\n",
        "host_vectors_x = np.random.uniform(low=-L, high=L, size=globalSize,).astype(np.float32)\n",
        "np.random.seed(11)\n",
        "host_vectors_y = np.random.uniform(low=-L, high=L, size=globalSize).astype(np.float32)\n",
        "host_labels = np.zeros(globalSize).astype(np.float32)\n",
        "host_group_sums = np.zeros(groups).astype(np.float32)\n",
        "\n",
        "plataforms = cl.get_platforms()\n",
        "print(\"Plataformas: \", plataforms)\n",
        "\n",
        "gpu_devices = plataforms[0].get_devices(cl.device_type.GPU)\n",
        "print(\"Devices: \", gpu_devices)\n",
        "\n",
        "context = cl.Context(gpu_devices)\n",
        "queue = cl.CommandQueue(context)\n",
        "\n",
        "device_vectors_x = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf= host_vectors_x)\n",
        "device_vectors_y = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf= host_vectors_y)\n",
        "\n",
        "\n",
        "device_group_sums = cl.Buffer(context, cl.mem_flags.WRITE_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf= host_group_sums)\n",
        "device_labels = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, host_labels.nbytes)\n",
        "\n",
        "print(f\"Iniciando algoritmo de Monte Carlo con K={K}\")\n",
        "print(f\"Redondeando K a {globalSize}\")\n",
        "print(f\"Se generarán {groups} grupos con {localSize} hilos cada uno\\n\")\n",
        "\n",
        "program = cl.Program(context, \n",
        "\"\"\"\n",
        "__kernel void mandelbrot(\n",
        "    __global float *vectors_x,\n",
        "     __global float *vectors_y,\n",
        "     __global float *device_labels,\n",
        "      __global float *group_sums)\n",
        "{\n",
        "  int global_id = get_global_id(0);\n",
        "  int local_id = get_local_id(0);\n",
        "  int group_id = get_group_id(0); \n",
        "  int local_size = get_local_size(0);\n",
        "\n",
        "  float c_x = vectors_x[global_id];\n",
        "  float c_y = vectors_y[global_id];\n",
        "\n",
        "  float z_nx = 0;\n",
        "  float z_ny = 0;\n",
        "  int i = 0;\n",
        "  float norm_zn = 0;\n",
        "  float u, v;\n",
        "  \n",
        "  while (i < 10000)\n",
        "\t{\n",
        "\t\t// Primero obtenemos el z_(n+1)\n",
        "    u = (z_nx * z_nx) - (z_ny * z_ny);\n",
        "    v =  2 * z_nx * z_ny;\n",
        "    z_nx = u + c_x;\n",
        "    z_ny = v + c_y;\n",
        "\n",
        "    norm_zn = (z_nx * z_nx) + (z_ny * z_ny);\n",
        "    if (norm_zn >= 4)\n",
        "\t  {\n",
        "\t\t  i = 1000000;\n",
        "      device_labels[global_id] = 1;\n",
        "\t  }\n",
        "    i += 1;\n",
        "\t}\n",
        "\n",
        "  // Ahora hacemos la reducción\n",
        "  \n",
        "  if(local_id == 0){\n",
        "    for (int j = 0; j < local_size; j++) {\n",
        "      group_sums[group_id] += device_labels[global_id + j];\n",
        "    }\n",
        "  }\n",
        "\n",
        "}\n",
        "\"\"\"\n",
        ").build()\n",
        "\n",
        "\n",
        "t0_GPU = tm.time()\n",
        "program.mandelbrot(queue, (globalSize,), (localSize,) , device_vectors_x, device_vectors_y, device_labels, device_group_sums)\n",
        "cl.enqueue_copy(queue, host_group_sums, device_group_sums)\n",
        "mandelbrot_complement = float(np.sum(host_group_sums))\n",
        "ratio = np.float32((globalSize - mandelbrot_complement)/ globalSize)\n",
        "area = 4*L*L*ratio \n",
        "t1_GPU = tm.time()\n",
        "print(f\"\\nAproximación superficie: {area}\")\n",
        "print(f\"Error: {np.abs(area-1.50659177) }\")\n",
        "print(f\"Tiempo de ejecución: {t1_GPU-t0_GPU}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plataformas:  [<pyopencl.Platform 'NVIDIA CUDA' at 0x3d9e760>]\n",
            "Devices:  [<pyopencl.Device 'Tesla T4' on 'NVIDIA CUDA' at 0x3d9e800>]\n",
            "Iniciando algoritmo de Monte Carlo con K=10000\n",
            "Redondeando K a 10000\n",
            "Se generarán 125 grupos con 80 hilos cada uno\n",
            "\n",
            "\n",
            "Aproximación superficie: 1.5152000188827515\n",
            "Error: 0.008608248882751468\n",
            "Tiempo de ejecución: 0.0019130706787109375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}